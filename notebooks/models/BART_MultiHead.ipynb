{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../../')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-SVX9K6RK_jk"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from transformers import TFBartModel, BartConfig\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate, GlobalAveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imSt6RsNLF19"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path, header=None, names=['event_type', 'agent_id', 'context'])\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    le_event = LabelEncoder()\n",
        "    le_agent = LabelEncoder()\n",
        "    le_context = LabelEncoder()\n",
        "\n",
        "    # Replace empty strings with a special token\n",
        "    df['context'] = df['context'].replace('', '<EMPTY>')\n",
        "\n",
        "    df['event_type_encoded'] = le_event.fit_transform(df['event_type'])\n",
        "    df['agent_id_encoded'] = le_agent.fit_transform(df['agent_id'])\n",
        "    df['context_encoded'] = le_context.fit_transform(df['context'])\n",
        "\n",
        "    return df, le_event, le_agent, le_context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ms5K8836LZ9F"
      },
      "outputs": [],
      "source": [
        "# Create sequences\n",
        "def create_sequences(df, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(len(df) - sequence_length):\n",
        "        seq = df.iloc[i:i+sequence_length]\n",
        "        target = df.iloc[i+sequence_length]\n",
        "\n",
        "        sequences.append(seq[['event_type_encoded', 'agent_id_encoded', 'context_encoded']].values)\n",
        "        targets.append([target['event_type_encoded'], target['agent_id_encoded'], target['context_encoded']])\n",
        "\n",
        "    return np.array(sequences), np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o1a9OKyrLmoi"
      },
      "outputs": [],
      "source": [
        "def build_model(input_shape, num_classes):\n",
        "    # BART configuration\n",
        "    config = BartConfig.from_pretrained('facebook/bart-base')\n",
        "    config.output_hidden_states = True\n",
        "\n",
        "    # Input layers\n",
        "    input_event = Input(shape=(input_shape[0], 1))\n",
        "    input_agent = Input(shape=(input_shape[0], 1))\n",
        "    input_context = Input(shape=(input_shape[0], 1))\n",
        "\n",
        "    # Concatenate inputs\n",
        "    concat = Concatenate()([input_event, input_agent, input_context])\n",
        "\n",
        "    # BART layer\n",
        "    bart_model = TFBartModel.from_pretrained('facebook/bart-base', config=config)\n",
        "    bart_output = bart_model(concat)[0]  # Use the last hidden state\n",
        "\n",
        "    # Global Average Pooling\n",
        "    pooled_output = GlobalAveragePooling1D()(bart_output)\n",
        "\n",
        "    # Output layers\n",
        "    output_event = Dense(num_classes[0], activation='softmax', name='event_type')(pooled_output)\n",
        "    output_agent = Dense(num_classes[1], activation='softmax', name='agent_id')(pooled_output)\n",
        "    output_context = Dense(num_classes[2], activation='softmax', name='context')(pooled_output)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[input_event, input_agent, input_context],\n",
        "                  outputs=[output_event, output_agent, output_context])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy', 'accuracy', 'accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_next_moves(model, initial_sequence, le_event, le_agent, le_context, n_predictions):\n",
        "    predictions = []\n",
        "    current_sequence = initial_sequence.copy()\n",
        "\n",
        "    for _ in range(n_predictions):\n",
        "        custom_event = current_sequence[:, 0].reshape(1, -1, 1)\n",
        "        custom_agent = current_sequence[:, 1].reshape(1, -1, 1)\n",
        "        custom_context = current_sequence[:, 2].reshape(1, -1, 1)\n",
        "\n",
        "        prediction = model.predict([custom_event, custom_agent, custom_context])\n",
        "\n",
        "        predicted_event = le_event.inverse_transform([np.argmax(prediction[0])])[0]\n",
        "        predicted_agent = le_agent.inverse_transform([np.argmax(prediction[1])])[0]\n",
        "        predicted_context = le_context.inverse_transform([np.argmax(prediction[2])])[0]\n",
        "\n",
        "        predicted_context = '' if predicted_context == '<EMPTY>' else predicted_context\n",
        "\n",
        "        predictions.append((predicted_event, predicted_agent, predicted_context))\n",
        "\n",
        "        # Update the sequence for the next prediction\n",
        "        new_row = np.array([[\n",
        "            le_event.transform([predicted_event])[0],\n",
        "            le_agent.transform([predicted_agent])[0],\n",
        "            le_context.transform([predicted_context if predicted_context != '' else '<EMPTY>'])[0]\n",
        "        ]])\n",
        "        current_sequence = np.vstack((current_sequence[1:], new_row))\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = load_data('..\\..\\data\\processed/games/tic-tac-toe/100_single_agent.csv')\n",
        "df, le_event, le_agent, le_context = preprocess_data(df)\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 10  # Changed to 10\n",
        "X, y = create_sequences(df, sequence_length)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBartModel.\n",
            "\n",
            "All the weights of TFBartModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartModel for predictions without further training.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Exception encountered when calling layer 'tf_bart_model' (type TFBartModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bart_model' (type TFBartModel):\n  • input_ids=<KerasTensor shape=(None, 10, 3), dtype=float32, sparse=False, name=keras_tensor_3>\n  • attention_mask=None\n  • decoder_input_ids=None\n  • decoder_attention_mask=None\n  • decoder_position_ids=None\n  • head_mask=None\n  • decoder_head_mask=None\n  • cross_attn_head_mask=None\n  • encoder_outputs=None\n  • past_key_values=None\n  • inputs_embeds=None\n  • decoder_inputs_embeds=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False\n  • kwargs=<class 'inspect._empty'>",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m y_test_context \u001b[38;5;241m=\u001b[39m to_categorical(y_test[:, \u001b[38;5;241m2\u001b[39m], num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(le_context\u001b[38;5;241m.\u001b[39mclasses_))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Build and train model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mle_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mle_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mle_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mfit([X_train_event, X_train_agent, X_train_context],\n\u001b[0;32m     23\u001b[0m             [y_train_event, y_train_agent, y_train_context],\n\u001b[0;32m     24\u001b[0m             validation_data\u001b[38;5;241m=\u001b[39m([X_test_event, X_test_agent, X_test_context],\n\u001b[0;32m     25\u001b[0m                             [y_test_event, y_test_agent, y_test_context]),\n\u001b[0;32m     26\u001b[0m             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(input_shape, num_classes)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# BART layer\u001b[39;00m\n\u001b[0;32m     15\u001b[0m bart_model \u001b[38;5;241m=\u001b[39m TFBartModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfacebook/bart-base\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[1;32m---> 16\u001b[0m bart_output \u001b[38;5;241m=\u001b[39m \u001b[43mbart_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Use the last hidden state\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Global Average Pooling\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m GlobalAveragePooling1D()(bart_output)\n",
            "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
            "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\modeling_tf_utils.py:566\u001b[0m, in \u001b[0;36minput_processing\u001b[1;34m(func, config, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m         output[main_input_name] \u001b[38;5;241m=\u001b[39m main_input\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 566\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(main_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m         )\n\u001b[0;32m    571\u001b[0m \u001b[38;5;66;03m# Populates any unspecified argument with their default value, according to the signature.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m parameter_names:\n",
            "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bart_model' (type TFBartModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for input_ids.\n\nCall arguments received by layer 'tf_bart_model' (type TFBartModel):\n  • input_ids=<KerasTensor shape=(None, 10, 3), dtype=float32, sparse=False, name=keras_tensor_3>\n  • attention_mask=None\n  • decoder_input_ids=None\n  • decoder_attention_mask=None\n  • decoder_position_ids=None\n  • head_mask=None\n  • decoder_head_mask=None\n  • cross_attn_head_mask=None\n  • encoder_outputs=None\n  • past_key_values=None\n  • inputs_embeds=None\n  • decoder_inputs_embeds=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False\n  • kwargs=<class 'inspect._empty'>"
          ]
        }
      ],
      "source": [
        "# Prepare inputs and outputs\n",
        "X_train_event = X_train[:, :, 0].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_train_agent = X_train[:, :, 1].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_train_context = X_train[:, :, 2].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "\n",
        "X_test_event = X_test[:, :, 0].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "X_test_agent = X_test[:, :, 1].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "X_test_context = X_test[:, :, 2].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "y_train_event = to_categorical(y_train[:, 0], num_classes=len(le_event.classes_))\n",
        "y_train_agent = to_categorical(y_train[:, 1], num_classes=len(le_agent.classes_))\n",
        "y_train_context = to_categorical(y_train[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "y_test_event = to_categorical(y_test[:, 0], num_classes=len(le_event.classes_))\n",
        "y_test_agent = to_categorical(y_test[:, 1], num_classes=len(le_agent.classes_))\n",
        "y_test_context = to_categorical(y_test[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "# Build and train model\n",
        "model = build_model(X_train_event.shape[1:],\n",
        "                    [len(le_event.classes_), len(le_agent.classes_), len(le_context.classes_)])\n",
        "\n",
        "model.fit([X_train_event, X_train_agent, X_train_context],\n",
        "            [y_train_event, y_train_agent, y_train_context],\n",
        "            validation_data=([X_test_event, X_test_agent, X_test_context],\n",
        "                            [y_test_event, y_test_agent, y_test_context]),\n",
        "            epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate model\n",
        "results = model.evaluate(\n",
        "        [X_test_event, X_test_agent, X_test_context],\n",
        "        [y_test_event, y_test_agent, y_test_context]\n",
        "    )\n",
        "    \n",
        "    # Print evaluation results\n",
        "print(\"Test Results:\")\n",
        "for metric_name, value in zip(model.metrics_names, results):\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict next moves for a random sequence from the dataset\n",
        "import random\n",
        "k = 10  # size of initial sequence, changed to 10\n",
        "n_predictions = 15  # number of moves to predict\n",
        "\n",
        "# Select a random sequence from the dataset\n",
        "random_index = random.randint(0, len(X) - 1)\n",
        "initial_sequence = X[random_index]\n",
        "\n",
        "predictions = predict_next_moves(model, initial_sequence, le_event, le_agent, le_context, n_predictions)\n",
        "\n",
        "print(\"Initial sequence:\")\n",
        "for i in range(k):\n",
        "    event = le_event.inverse_transform([initial_sequence[i, 0]])[0]\n",
        "    agent = le_agent.inverse_transform([initial_sequence[i, 1]])[0]\n",
        "    context = le_context.inverse_transform([initial_sequence[i, 2]])[0]\n",
        "    context = '' if context == '<EMPTY>' else context\n",
        "    print(f\"{event}, {agent}, {context}\")\n",
        "\n",
        "print(\"\\nPredicted moves:\")\n",
        "for i, (event, agent, context) in enumerate(predictions, 1):\n",
        "    print(f\"Move {i}: {event}, {agent}, {context}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume `model` is your Keras model\n",
        "model.save('../../results/models/BART.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume `model` is your Keras model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcK-xKpcLpE3",
        "outputId": "fa0d510c-a30b-4b0f-ee8d-18a3ad21fc21"
      },
      "outputs": [],
      "source": [
        "# # Main pipeline\n",
        "\n",
        "# # Load and preprocess data\n",
        "# df = load_data('..\\..\\data\\processed/games/tic-tac-toe/50k_single_agent.csv')\n",
        "# df, le_event, le_agent, le_context = preprocess_data(df)\n",
        "\n",
        "# # Create sequences\n",
        "# sequence_length = 10\n",
        "# X, y = create_sequences(df, sequence_length)\n",
        "\n",
        "# # Split data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Prepare inputs and outputs\n",
        "# X_train_event = X_train[:, :, 0].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "# X_train_agent = X_train[:, :, 1].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "# X_train_context = X_train[:, :, 2].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "\n",
        "# X_test_event = X_test[:, :, 0].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "# X_test_agent = X_test[:, :, 1].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "# X_test_context = X_test[:, :, 2].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# y_train_event = to_categorical(y_train[:, 0], num_classes=len(le_event.classes_))\n",
        "# y_train_agent = to_categorical(y_train[:, 1], num_classes=len(le_agent.classes_))\n",
        "# y_train_context = to_categorical(y_train[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "# y_test_event = to_categorical(y_test[:, 0], num_classes=len(le_event.classes_))\n",
        "# y_test_agent = to_categorical(y_test[:, 1], num_classes=len(le_agent.classes_))\n",
        "# y_test_context = to_categorical(y_test[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "# # Build and train model\n",
        "# model = build_model(X_train_event.shape[1:],\n",
        "#                     [len(le_event.classes_), len(le_agent.classes_), len(le_context.classes_)])\n",
        "\n",
        "# model.fit([X_train_event, X_train_agent, X_train_context],\n",
        "#             [y_train_event, y_train_agent, y_train_context],\n",
        "#             validation_data=([X_test_event, X_test_agent, X_test_context],\n",
        "#                             [y_test_event, y_test_agent, y_test_context]),\n",
        "#             epochs=50, batch_size=32)\n",
        "\n",
        "# # Evaluate model\n",
        "# results = model.evaluate(\n",
        "#         [X_test_event, X_test_agent, X_test_context],\n",
        "#         [y_test_event, y_test_agent, y_test_context]\n",
        "#     )\n",
        "    \n",
        "#     # Print evaluation results\n",
        "# print(\"Test Results:\")\n",
        "# for metric_name, value in zip(model.metrics_names, results):\n",
        "#         print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbJZ_39ML1pN",
        "outputId": "51c1ab10-dca6-457d-c3cd-187e207914ee"
      },
      "outputs": [],
      "source": [
        "# custom_log = np.array([\n",
        "#         [le_event.transform(['GAME_START'])[0], le_agent.transform(['system'])[0], le_context.transform(['New Game'])[0]],\n",
        "#         [le_event.transform(['MOVE'])[0], le_agent.transform(['X'])[0], le_context.transform(['2,2'])[0]],\n",
        "#         [le_event.transform(['MOVE'])[0], le_agent.transform(['O'])[0], le_context.transform(['2,0'])[0]]\n",
        "#     ])\n",
        "\n",
        "# # Pad the custom log to match the sequence length of 10\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#     # Pad the custom log to match the sequence length of 10\n",
        "#     if custom_log.shape[0] < sequence_length:\n",
        "#         padding = np.zeros((sequence_length - custom_log.shape[0], 3), dtype=int)\n",
        "#         custom_log_padded = np.vstack((padding, custom_log))\n",
        "#     else:\n",
        "#         custom_log_padded = custom_log[-sequence_length:]  # Keep only the last sequence_length elements\n",
        "\n",
        "#     custom_event = custom_log_padded[:, 0].reshape(1, sequence_length, 1)\n",
        "#     custom_agent = custom_log_padded[:, 1].reshape(1, sequence_length, 1)\n",
        "#     custom_context = custom_log_padded[:, 2].reshape(1, sequence_length, 1)\n",
        "\n",
        "#     predictions = model.predict([custom_event, custom_agent, custom_context])\n",
        "\n",
        "#     predicted_event = le_event.inverse_transform([np.argmax(predictions[0])])\n",
        "#     predicted_agent = le_agent.inverse_transform([np.argmax(predictions[1])])\n",
        "#     predicted_context = le_context.inverse_transform([np.argmax(predictions[2])])\n",
        "\n",
        "#     print(f\"Predicted next move: {predicted_event[0]}, {predicted_agent[0]}, {predicted_context[0]}\")\n",
        "#     custom_log = np.vstack((custom_log, [le_event.transform([predicted_event[0]])[0], le_agent.transform([predicted_agent[0]])[0], le_context.transform([predicted_context[0]])[0]]))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBEbcRDDNHQl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
