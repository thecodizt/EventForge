{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('../../')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-SVX9K6RK_jk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Concatenate,Bidirectional, Attention\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imSt6RsNLF19"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path, header=None, names=['event_type', 'agent_id', 'context'])\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df):\n",
        "    le_event = LabelEncoder()\n",
        "    le_agent = LabelEncoder()\n",
        "    le_context = LabelEncoder()\n",
        "\n",
        "    # Replace empty strings with a special token\n",
        "    df['context'] = df['context'].replace('', '<EMPTY>')\n",
        "\n",
        "    df['event_type_encoded'] = le_event.fit_transform(df['event_type'])\n",
        "    df['agent_id_encoded'] = le_agent.fit_transform(df['agent_id'])\n",
        "    df['context_encoded'] = le_context.fit_transform(df['context'])\n",
        "\n",
        "    return df, le_event, le_agent, le_context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ms5K8836LZ9F"
      },
      "outputs": [],
      "source": [
        "# Create sequences\n",
        "def create_sequences(df, sequence_length):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "\n",
        "    for i in range(len(df) - sequence_length):\n",
        "        seq = df.iloc[i:i+sequence_length]\n",
        "        target = df.iloc[i+sequence_length]\n",
        "\n",
        "        sequences.append(seq[['event_type_encoded', 'agent_id_encoded', 'context_encoded']].values)\n",
        "        targets.append([target['event_type_encoded'], target['agent_id_encoded'], target['context_encoded']])\n",
        "\n",
        "    return np.array(sequences), np.array(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o1a9OKyrLmoi"
      },
      "outputs": [],
      "source": [
        "# Build model\n",
        "def build_model(input_shape, num_classes):\n",
        "    input_event = Input(shape=(input_shape[0], 1))\n",
        "    input_agent = Input(shape=(input_shape[0], 1))\n",
        "    input_context = Input(shape=(input_shape[0], 1))\n",
        "\n",
        "    concat = Concatenate()([input_event, input_agent, input_context])\n",
        "\n",
        "    lstm = Bidirectional(LSTM(64, return_sequences=True))(concat)\n",
        "    attention = Attention()([lstm, lstm])\n",
        "    lstm_out = LSTM(64, return_sequences=False)(attention)\n",
        "\n",
        "    output_event = Dense(num_classes[0], activation='softmax', name='event_type')(lstm_out)\n",
        "    output_agent = Dense(num_classes[1], activation='softmax', name='agent_id')(lstm_out)\n",
        "    output_context = Dense(num_classes[2], activation='softmax', name='context')(lstm_out)\n",
        "\n",
        "    model = Model(inputs=[input_event, input_agent, input_context],\n",
        "                  outputs=[output_event, output_agent, output_context])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy','accuracy','accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_next_moves(model, initial_sequence, le_event, le_agent, le_context, n_predictions):\n",
        "    predictions = []\n",
        "    current_sequence = initial_sequence.copy()\n",
        "\n",
        "    for _ in range(n_predictions):\n",
        "        custom_event = current_sequence[:, 0].reshape(1, -1, 1)\n",
        "        custom_agent = current_sequence[:, 1].reshape(1, -1, 1)\n",
        "        custom_context = current_sequence[:, 2].reshape(1, -1, 1)\n",
        "\n",
        "        prediction = model.predict([custom_event, custom_agent, custom_context])\n",
        "\n",
        "        predicted_event = le_event.inverse_transform([np.argmax(prediction[0])])[0]\n",
        "        predicted_agent = le_agent.inverse_transform([np.argmax(prediction[1])])[0]\n",
        "        predicted_context = le_context.inverse_transform([np.argmax(prediction[2])])[0]\n",
        "\n",
        "        predicted_context = '' if predicted_context == '<EMPTY>' else predicted_context\n",
        "\n",
        "        predictions.append((predicted_event, predicted_agent, predicted_context))\n",
        "\n",
        "        # Update the sequence for the next prediction\n",
        "        new_row = np.array([[\n",
        "            le_event.transform([predicted_event])[0],\n",
        "            le_agent.transform([predicted_agent])[0],\n",
        "            le_context.transform([predicted_context if predicted_context != '' else '<EMPTY>'])[0]\n",
        "        ]])\n",
        "        current_sequence = np.vstack((current_sequence[1:], new_row))\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "df = load_data('..\\..\\data\\processed/games/tic-tac-toe/10k_single_agent.csv')\n",
        "df, le_event, le_agent, le_context = preprocess_data(df)\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 10  # Changed to 10\n",
        "X, y = create_sequences(df, sequence_length)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - agent_id_accuracy: 0.7363 - context_accuracy: 0.2246 - event_type_accuracy: 0.9037 - loss: 2.8328 - val_agent_id_accuracy: 0.9334 - val_context_accuracy: 0.2744 - val_event_type_accuracy: 0.9334 - val_loss: 2.1885\n",
            "Epoch 2/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - agent_id_accuracy: 0.9289 - context_accuracy: 0.2745 - event_type_accuracy: 0.9289 - loss: 2.1881 - val_agent_id_accuracy: 0.9334 - val_context_accuracy: 0.2759 - val_event_type_accuracy: 0.9334 - val_loss: 2.1580\n",
            "Epoch 3/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - agent_id_accuracy: 0.9332 - context_accuracy: 0.2761 - event_type_accuracy: 0.9337 - loss: 2.1567 - val_agent_id_accuracy: 0.9332 - val_context_accuracy: 0.2879 - val_event_type_accuracy: 0.9334 - val_loss: 2.1137\n",
            "Epoch 4/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - agent_id_accuracy: 0.9308 - context_accuracy: 0.2888 - event_type_accuracy: 0.9316 - loss: 2.1063 - val_agent_id_accuracy: 0.9330 - val_context_accuracy: 0.2834 - val_event_type_accuracy: 0.9334 - val_loss: 2.0861\n",
            "Epoch 5/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - agent_id_accuracy: 0.9319 - context_accuracy: 0.2997 - event_type_accuracy: 0.9324 - loss: 2.0645 - val_agent_id_accuracy: 0.9329 - val_context_accuracy: 0.2968 - val_event_type_accuracy: 0.9334 - val_loss: 2.0474\n",
            "Epoch 6/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - agent_id_accuracy: 0.9329 - context_accuracy: 0.3079 - event_type_accuracy: 0.9331 - loss: 2.0240 - val_agent_id_accuracy: 0.9324 - val_context_accuracy: 0.3033 - val_event_type_accuracy: 0.9334 - val_loss: 2.0288\n",
            "Epoch 7/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - agent_id_accuracy: 0.9321 - context_accuracy: 0.3129 - event_type_accuracy: 0.9322 - loss: 1.9987 - val_agent_id_accuracy: 0.9325 - val_context_accuracy: 0.3086 - val_event_type_accuracy: 0.9335 - val_loss: 1.9924\n",
            "Epoch 8/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - agent_id_accuracy: 0.9324 - context_accuracy: 0.3200 - event_type_accuracy: 0.9328 - loss: 1.9632 - val_agent_id_accuracy: 0.9347 - val_context_accuracy: 0.3208 - val_event_type_accuracy: 0.9354 - val_loss: 1.9504\n",
            "Epoch 9/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - agent_id_accuracy: 0.9352 - context_accuracy: 0.3253 - event_type_accuracy: 0.9356 - loss: 1.9256 - val_agent_id_accuracy: 0.9400 - val_context_accuracy: 0.3208 - val_event_type_accuracy: 0.9402 - val_loss: 1.8922\n",
            "Epoch 10/10\n",
            "\u001b[1m2405/2405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - agent_id_accuracy: 0.9414 - context_accuracy: 0.3312 - event_type_accuracy: 0.9414 - loss: 1.8801 - val_agent_id_accuracy: 0.9418 - val_context_accuracy: 0.3298 - val_event_type_accuracy: 0.9429 - val_loss: 1.8503\n",
            "\u001b[1m602/602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - agent_id_accuracy: 0.9408 - context_accuracy: 0.3308 - event_type_accuracy: 0.9423 - loss: 1.8509\n",
            "Test Results:\n",
            "loss: 1.8503\n",
            "compile_metrics: 0.9418\n"
          ]
        }
      ],
      "source": [
        "# Prepare inputs and outputs\n",
        "X_train_event = X_train[:, :, 0].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_train_agent = X_train[:, :, 1].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_train_context = X_train[:, :, 2].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "\n",
        "X_test_event = X_test[:, :, 0].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "X_test_agent = X_test[:, :, 1].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "X_test_context = X_test[:, :, 2].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "y_train_event = to_categorical(y_train[:, 0], num_classes=len(le_event.classes_))\n",
        "y_train_agent = to_categorical(y_train[:, 1], num_classes=len(le_agent.classes_))\n",
        "y_train_context = to_categorical(y_train[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "y_test_event = to_categorical(y_test[:, 0], num_classes=len(le_event.classes_))\n",
        "y_test_agent = to_categorical(y_test[:, 1], num_classes=len(le_agent.classes_))\n",
        "y_test_context = to_categorical(y_test[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "# Build and train model\n",
        "model = build_model(X_train_event.shape[1:],\n",
        "                    [len(le_event.classes_), len(le_agent.classes_), len(le_context.classes_)])\n",
        "\n",
        "model.fit([X_train_event, X_train_agent, X_train_context],\n",
        "            [y_train_event, y_train_agent, y_train_context],\n",
        "            validation_data=([X_test_event, X_test_agent, X_test_context],\n",
        "                            [y_test_event, y_test_agent, y_test_context]),\n",
        "            epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate model\n",
        "results = model.evaluate(\n",
        "        [X_test_event, X_test_agent, X_test_context],\n",
        "        [y_test_event, y_test_agent, y_test_context]\n",
        "    )\n",
        "    \n",
        "    # Print evaluation results\n",
        "print(\"Test Results:\")\n",
        "for metric_name, value in zip(model.metrics_names, results):\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Initial sequence:\n",
            "MOVE, O, 2,2\n",
            "MOVE, X, 1,1\n",
            "MOVE, O, 1,0\n",
            "MOVE, X, 0,0\n",
            "GAME_END, system, draw\n",
            "GAME_START, system, New Game\n",
            "MOVE, X, 1,0\n",
            "MOVE, O, 2,1\n",
            "MOVE, X, 2,2\n",
            "MOVE, O, 2,0\n",
            "\n",
            "Predicted moves:\n",
            "Move 1: MOVE, X, 0,1\n",
            "Move 2: MOVE, O, 1,1\n",
            "Move 3: MOVE, X, 0,0\n",
            "Move 4: MOVE, O, 1,2\n",
            "Move 5: MOVE, X, 0,2\n",
            "Move 6: GAME_END, system, X\n",
            "Move 7: GAME_START, system, New Game\n",
            "Move 8: MOVE, X, 2,0\n",
            "Move 9: MOVE, O, 2,2\n",
            "Move 10: MOVE, X, 0,2\n",
            "Move 11: MOVE, O, 0,0\n",
            "Move 12: MOVE, X, 1,0\n",
            "Move 13: MOVE, O, 0,1\n",
            "Move 14: MOVE, X, 2,1\n",
            "Move 15: MOVE, O, X\n"
          ]
        }
      ],
      "source": [
        "# Predict next moves for a random sequence from the dataset\n",
        "import random\n",
        "k = 10  # size of initial sequence, changed to 10\n",
        "n_predictions = 15  # number of moves to predict\n",
        "\n",
        "# Select a random sequence from the dataset\n",
        "random_index = random.randint(0, len(X) - 1)\n",
        "initial_sequence = X[random_index]\n",
        "\n",
        "predictions = predict_next_moves(model, initial_sequence, le_event, le_agent, le_context, n_predictions)\n",
        "\n",
        "print(\"Initial sequence:\")\n",
        "for i in range(k):\n",
        "    event = le_event.inverse_transform([initial_sequence[i, 0]])[0]\n",
        "    agent = le_agent.inverse_transform([initial_sequence[i, 1]])[0]\n",
        "    context = le_context.inverse_transform([initial_sequence[i, 2]])[0]\n",
        "    context = '' if context == '<EMPTY>' else context\n",
        "    print(f\"{event}, {agent}, {context}\")\n",
        "\n",
        "print(\"\\nPredicted moves:\")\n",
        "for i, (event, agent, context) in enumerate(predictions, 1):\n",
        "    print(f\"Move {i}: {event}, {agent}, {context}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Unable to synchronously create dataset (name already exists)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assume `model` is your Keras model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../results/models/BiDir_LSTM_ATTN.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
            "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[1;32m--> 163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m    166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n",
            "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mh5py\\h5d.pyx:137\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Unable to synchronously create dataset (name already exists)"
          ]
        }
      ],
      "source": [
        "# Assume `model` is your Keras model\n",
        "model.save('../../results/models/BiDir_LSTM_ATTN.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assume `model` is your Keras model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcK-xKpcLpE3",
        "outputId": "fa0d510c-a30b-4b0f-ee8d-18a3ad21fc21"
      },
      "outputs": [],
      "source": [
        "# # Main pipeline\n",
        "\n",
        "# # Load and preprocess data\n",
        "# df = load_data('..\\..\\data\\processed/games/tic-tac-toe/50k_single_agent.csv')\n",
        "# df, le_event, le_agent, le_context = preprocess_data(df)\n",
        "\n",
        "# # Create sequences\n",
        "# sequence_length = 10\n",
        "# X, y = create_sequences(df, sequence_length)\n",
        "\n",
        "# # Split data\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Prepare inputs and outputs\n",
        "# X_train_event = X_train[:, :, 0].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "# X_train_agent = X_train[:, :, 1].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "# X_train_context = X_train[:, :, 2].reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "\n",
        "# X_test_event = X_test[:, :, 0].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "# X_test_agent = X_test[:, :, 1].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "# X_test_context = X_test[:, :, 2].reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# y_train_event = to_categorical(y_train[:, 0], num_classes=len(le_event.classes_))\n",
        "# y_train_agent = to_categorical(y_train[:, 1], num_classes=len(le_agent.classes_))\n",
        "# y_train_context = to_categorical(y_train[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "# y_test_event = to_categorical(y_test[:, 0], num_classes=len(le_event.classes_))\n",
        "# y_test_agent = to_categorical(y_test[:, 1], num_classes=len(le_agent.classes_))\n",
        "# y_test_context = to_categorical(y_test[:, 2], num_classes=len(le_context.classes_))\n",
        "\n",
        "# # Build and train model\n",
        "# model = build_model(X_train_event.shape[1:],\n",
        "#                     [len(le_event.classes_), len(le_agent.classes_), len(le_context.classes_)])\n",
        "\n",
        "# model.fit([X_train_event, X_train_agent, X_train_context],\n",
        "#             [y_train_event, y_train_agent, y_train_context],\n",
        "#             validation_data=([X_test_event, X_test_agent, X_test_context],\n",
        "#                             [y_test_event, y_test_agent, y_test_context]),\n",
        "#             epochs=50, batch_size=32)\n",
        "\n",
        "# # Evaluate model\n",
        "# results = model.evaluate(\n",
        "#         [X_test_event, X_test_agent, X_test_context],\n",
        "#         [y_test_event, y_test_agent, y_test_context]\n",
        "#     )\n",
        "    \n",
        "#     # Print evaluation results\n",
        "# print(\"Test Results:\")\n",
        "# for metric_name, value in zip(model.metrics_names, results):\n",
        "#         print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbJZ_39ML1pN",
        "outputId": "51c1ab10-dca6-457d-c3cd-187e207914ee"
      },
      "outputs": [],
      "source": [
        "# custom_log = np.array([\n",
        "#         [le_event.transform(['GAME_START'])[0], le_agent.transform(['system'])[0], le_context.transform(['New Game'])[0]],\n",
        "#         [le_event.transform(['MOVE'])[0], le_agent.transform(['X'])[0], le_context.transform(['2,2'])[0]],\n",
        "#         [le_event.transform(['MOVE'])[0], le_agent.transform(['O'])[0], le_context.transform(['2,0'])[0]]\n",
        "#     ])\n",
        "\n",
        "# # Pad the custom log to match the sequence length of 10\n",
        "\n",
        "\n",
        "# for _ in range(5):\n",
        "#     # Pad the custom log to match the sequence length of 10\n",
        "#     if custom_log.shape[0] < sequence_length:\n",
        "#         padding = np.zeros((sequence_length - custom_log.shape[0], 3), dtype=int)\n",
        "#         custom_log_padded = np.vstack((padding, custom_log))\n",
        "#     else:\n",
        "#         custom_log_padded = custom_log[-sequence_length:]  # Keep only the last sequence_length elements\n",
        "\n",
        "#     custom_event = custom_log_padded[:, 0].reshape(1, sequence_length, 1)\n",
        "#     custom_agent = custom_log_padded[:, 1].reshape(1, sequence_length, 1)\n",
        "#     custom_context = custom_log_padded[:, 2].reshape(1, sequence_length, 1)\n",
        "\n",
        "#     predictions = model.predict([custom_event, custom_agent, custom_context])\n",
        "\n",
        "#     predicted_event = le_event.inverse_transform([np.argmax(predictions[0])])\n",
        "#     predicted_agent = le_agent.inverse_transform([np.argmax(predictions[1])])\n",
        "#     predicted_context = le_context.inverse_transform([np.argmax(predictions[2])])\n",
        "\n",
        "#     print(f\"Predicted next move: {predicted_event[0]}, {predicted_agent[0]}, {predicted_context[0]}\")\n",
        "#     custom_log = np.vstack((custom_log, [le_event.transform([predicted_event[0]])[0], le_agent.transform([predicted_agent[0]])[0], le_context.transform([predicted_context[0]])[0]]))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBEbcRDDNHQl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
